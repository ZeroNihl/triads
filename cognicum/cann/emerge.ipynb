{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Requirement already satisfied: ezdxf in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.3.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from ezdxf) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from ezdxf) (4.9.0)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from ezdxf) (2.1.1)\n",
      "Requirement already satisfied: fonttools in /home/codespace/.local/lib/python3.12/site-packages (from ezdxf) (4.53.1)\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Requirement already satisfied: tinygrad in /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg (0.9.2)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from tinygrad) (2.1.1)\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Requirement already satisfied: loguru in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.7.2)\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Requirement already satisfied: colorama in /home/codespace/.local/lib/python3.12/site-packages (0.4.6)\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.66.5)\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Requirement already satisfied: plotly in /home/codespace/.local/lib/python3.12/site-packages (5.24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from plotly) (24.1)\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/codespace/.local/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from ipywidgets) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /home/codespace/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/codespace/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/codespace/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/codespace/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/codespace/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/codespace/.local/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/codespace/.local/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/codespace/.local/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/codespace/.local/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "   25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/2.3 MB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 34.5 MB/s eta 0:00:00\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/my_project-0.1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at /usr/local/python/3.12.1/lib/python3.12/site-packages/tinygrad-0.9.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (1.14.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /home/codespace/.local/lib/python3.12/site-packages (from scipy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ezdxf\n",
    "!pip install tinygrad\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install loguru\n",
    "!pip install colorama\n",
    "!pip install tqdm\n",
    "!pip install plotly\n",
    "!pip install ipywidgets\n",
    "!pip install scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import ezdxf\n",
    "\n",
    "\n",
    "# DXF export function\n",
    "def export_loss_to_dxf(loss_values, filename=\"loss_curve.dxf\"):\n",
    "    doc = ezdxf.new(dxfversion='R2010')\n",
    "    msp = doc.modelspace()\n",
    "    scale = 10\n",
    "    for i, loss in enumerate(loss_values[:-1]):\n",
    "        msp.add_line((i * scale, loss_values[i] * scale), ((i + 1) * scale, loss_values[i + 1] * scale))\n",
    "    doc.saveas(filename)\n",
    "    print(f\"Loss curve exported as {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor\n",
    "import numpy as np\n",
    "import tinygrad as tngrd\n",
    "\n",
    "class AttentionHead:\n",
    "    def __init__(self, embed_dim, head_dim, requires_grad=True, dtype=tngrd.dtypes.float16):\n",
    "        # Set dtype\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Initialize weights as Tensors with the specified dtype\n",
    "        self.W_q = Tensor.randn(embed_dim, head_dim, dtype=self.dtype, requires_grad=requires_grad)\n",
    "        self.W_k = Tensor.randn(embed_dim, head_dim, dtype=self.dtype, requires_grad=requires_grad)\n",
    "        self.W_v = Tensor.randn(embed_dim, head_dim, dtype=self.dtype, requires_grad=requires_grad)\n",
    "        self.W_o = Tensor.randn(head_dim, embed_dim, dtype=self.dtype, requires_grad=requires_grad)\n",
    "\n",
    "        # Initialize biases\n",
    "        self.b_q = Tensor.zeros(head_dim, dtype=self.dtype, requires_grad=requires_grad)\n",
    "        self.b_k = Tensor.zeros(head_dim, dtype=self.dtype, requires_grad=requires_grad)\n",
    "        self.b_v = Tensor.zeros(head_dim, dtype=self.dtype, requires_grad=requires_grad)\n",
    "        self.b_o = Tensor.zeros(embed_dim, dtype=self.dtype, requires_grad=requires_grad)\n",
    "\n",
    "        # Precompute the scale factor as a constant tensor\n",
    "        self.scale_factor = Tensor((head_dim ** 0.5), dtype=self.dtype, requires_grad=False)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Linear transformation for Q, K, V\n",
    "        Q = x @ self.W_q + self.b_q\n",
    "        K = x @ self.W_k + self.b_k\n",
    "        V = x @ self.W_v + self.b_v\n",
    "\n",
    "        # Dot product attention (QK^T)\n",
    "        scores = Q @ K.transpose(2, 1)\n",
    "        scaled_scores = scores / self.scale_factor\n",
    "        attention_weights = scaled_scores.softmax(-1)\n",
    "\n",
    "        # Attention output (Weighted sum of V)\n",
    "        attention_output = attention_weights @ V\n",
    "\n",
    "        # Final linear transformation\n",
    "        output = attention_output @ self.W_o + self.b_o\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_forward_pass (__main__.TestAttentionHead.test_forward_pass) ... ok\n",
      "test_gradient_check (__main__.TestAttentionHead.test_gradient_check) ... ok\n",
      "test_linear_projection_shapes (__main__.TestAttentionHead.test_linear_projection_shapes) ... ok\n",
      "test_training (__main__.TestAttentionHead.test_training) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: array(207.10237, dtype=float32)\n",
      "Loss: array(63.100628, dtype=float32)\n",
      "Loss: array(156.17815, dtype=float32)\n",
      "Loss: array(115.389, dtype=float32)\n",
      "Loss: array(90.95379, dtype=float32)\n",
      "Loss: array(110.74843, dtype=float32)\n",
      "Loss: array(46.058445, dtype=float32)\n",
      "Loss: array(59.310402, dtype=float32)\n",
      "Loss: array(84.15816, dtype=float32)\n",
      "Loss: array(58.05771, dtype=float32)\n",
      "Loss: array(80.72236, dtype=float32)\n",
      "Loss: array(35.15234, dtype=float32)\n",
      "Loss: array(42.367126, dtype=float32)\n",
      "Loss: array(44.041824, dtype=float32)\n",
      "Loss: array(51.60333, dtype=float32)\n",
      "Loss: array(39.01849, dtype=float32)\n",
      "Loss: array(28.87851, dtype=float32)\n",
      "Loss: array(49.33441, dtype=float32)\n",
      "Loss: array(42.728138, dtype=float32)\n",
      "Loss: array(38.990295, dtype=float32)\n",
      "Loss: array(42.628036, dtype=float32)\n",
      "Loss: array(19.084894, dtype=float32)\n",
      "Loss: array(37.17585, dtype=float32)\n",
      "Loss: array(13.000665, dtype=float32)\n",
      "Loss: array(14.316606, dtype=float32)\n",
      "Loss: array(15.760514, dtype=float32)\n",
      "Loss: array(20.19385, dtype=float32)\n",
      "Loss: array(20.020758, dtype=float32)\n",
      "Loss: array(15.122491, dtype=float32)\n",
      "Loss: array(13.419475, dtype=float32)\n",
      "Loss: array(20.598906, dtype=float32)\n",
      "Loss: array(10.693778, dtype=float32)\n",
      "Loss: array(17.780834, dtype=float32)\n",
      "Loss: array(21.61876, dtype=float32)\n",
      "Loss: array(17.462378, dtype=float32)\n",
      "Loss: array(14.1928215, dtype=float32)\n",
      "Loss: array(9.992226, dtype=float32)\n",
      "Loss: array(9.684103, dtype=float32)\n",
      "Loss: array(12.468604, dtype=float32)\n",
      "Loss: array(24.102926, dtype=float32)\n",
      "Loss: array(12.360608, dtype=float32)\n",
      "Loss: array(13.53702, dtype=float32)\n",
      "Loss: array(10.729913, dtype=float32)\n",
      "Loss: array(7.0492315, dtype=float32)\n",
      "Loss: array(9.295704, dtype=float32)\n",
      "Loss: array(8.939543, dtype=float32)\n",
      "Loss: array(10.998002, dtype=float32)\n",
      "Loss: array(8.505719, dtype=float32)\n",
      "Loss: array(5.221068, dtype=float32)\n",
      "Loss: array(7.6807194, dtype=float32)\n",
      "Loss: array(10.66114, dtype=float32)\n",
      "Loss: array(6.3119926, dtype=float32)\n",
      "Loss: array(7.923295, dtype=float32)\n",
      "Loss: array(5.0607305, dtype=float32)\n",
      "Loss: array(3.9486296, dtype=float32)\n",
      "Loss: array(6.9449477, dtype=float32)\n",
      "Loss: array(5.5994883, dtype=float32)\n",
      "Loss: array(6.665423, dtype=float32)\n",
      "Loss: array(5.786189, dtype=float32)\n",
      "Loss: array(6.197736, dtype=float32)\n",
      "Loss: array(8.457615, dtype=float32)\n",
      "Loss: array(6.0053444, dtype=float32)\n",
      "Loss: array(6.1865616, dtype=float32)\n",
      "Loss: array(4.182945, dtype=float32)\n",
      "Loss: array(5.429822, dtype=float32)\n",
      "Loss: array(4.735813, dtype=float32)\n",
      "Loss: array(6.0859303, dtype=float32)\n",
      "Loss: array(4.73158, dtype=float32)\n",
      "Loss: array(4.471598, dtype=float32)\n",
      "Loss: array(5.857303, dtype=float32)\n",
      "Loss: array(6.2909436, dtype=float32)\n",
      "Loss: array(4.249937, dtype=float32)\n",
      "Loss: array(3.410574, dtype=float32)\n",
      "Loss: array(3.8340993, dtype=float32)\n",
      "Loss: array(4.2670803, dtype=float32)\n",
      "Loss: array(5.0186014, dtype=float32)\n",
      "Loss: array(3.9546473, dtype=float32)\n",
      "Loss: array(4.1907783, dtype=float32)\n",
      "Loss: array(3.463992, dtype=float32)\n",
      "Loss: array(4.7933517, dtype=float32)\n",
      "Loss: array(4.055875, dtype=float32)\n",
      "Loss: array(3.671957, dtype=float32)\n",
      "Loss: array(4.3560486, dtype=float32)\n",
      "Loss: array(1.9460671, dtype=float32)\n",
      "Loss: array(3.545788, dtype=float32)\n",
      "Loss: array(2.6490173, dtype=float32)\n",
      "Loss: array(1.6663803, dtype=float32)\n",
      "Loss: array(4.8127227, dtype=float32)\n",
      "Loss: array(4.3042545, dtype=float32)\n",
      "Loss: array(3.3051386, dtype=float32)\n",
      "Loss: array(3.1093056, dtype=float32)\n",
      "Loss: array(3.8985934, dtype=float32)\n",
      "Loss: array(2.4311752, dtype=float32)\n",
      "Loss: array(2.6834602, dtype=float32)\n",
      "Loss: array(4.5993123, dtype=float32)\n",
      "Loss: array(2.9679387, dtype=float32)\n",
      "Loss: array(3.3747609, dtype=float32)\n",
      "Loss: array(2.3534038, dtype=float32)\n",
      "Loss: array(1.7148464, dtype=float32)\n",
      "Loss: array(3.2643645, dtype=float32)\n",
      "Loss: array(2.6949794, dtype=float32)\n",
      "Loss: array(3.7277257, dtype=float32)\n",
      "Loss: array(2.1008873, dtype=float32)\n",
      "Loss: array(2.908478, dtype=float32)\n",
      "Loss: array(2.6007626, dtype=float32)\n",
      "Loss: array(2.9341009, dtype=float32)\n",
      "Loss: array(2.2688463, dtype=float32)\n",
      "Loss: array(2.2710955, dtype=float32)\n",
      "Loss: array(1.8224229, dtype=float32)\n",
      "Loss: array(2.4204938, dtype=float32)\n",
      "Loss: array(2.1125436, dtype=float32)\n",
      "Loss: array(2.0719733, dtype=float32)\n",
      "Loss: array(1.9239458, dtype=float32)\n",
      "Loss: array(2.4081216, dtype=float32)\n",
      "Loss: array(1.6071634, dtype=float32)\n",
      "Loss: array(2.363634, dtype=float32)\n",
      "Loss: array(2.5895467, dtype=float32)\n",
      "Loss: array(1.9941628, dtype=float32)\n",
      "Loss: array(1.3643098, dtype=float32)\n",
      "Loss: array(1.7557619, dtype=float32)\n",
      "Loss: array(2.6078882, dtype=float32)\n",
      "Loss: array(2.7682967, dtype=float32)\n",
      "Loss: array(1.5171131, dtype=float32)\n",
      "Loss: array(2.182124, dtype=float32)\n",
      "Loss: array(2.9358702, dtype=float32)\n",
      "Loss: array(2.095129, dtype=float32)\n",
      "Loss: array(2.4719453, dtype=float32)\n",
      "Loss: array(1.7436643, dtype=float32)\n",
      "Loss: array(2.005342, dtype=float32)\n",
      "Loss: array(1.6104143, dtype=float32)\n",
      "Loss: array(2.2354927, dtype=float32)\n",
      "Loss: array(1.6695937, dtype=float32)\n",
      "Loss: array(1.3562081, dtype=float32)\n",
      "Loss: array(1.7990361, dtype=float32)\n",
      "Loss: array(1.7029383, dtype=float32)\n",
      "Loss: array(1.5489333, dtype=float32)\n",
      "Loss: array(1.9920872, dtype=float32)\n",
      "Loss: array(1.9765381, dtype=float32)\n",
      "Loss: array(1.4106506, dtype=float32)\n",
      "Loss: array(1.583163, dtype=float32)\n",
      "Loss: array(1.3008511, dtype=float32)\n",
      "Loss: array(1.5874463, dtype=float32)\n",
      "Loss: array(2.1385033, dtype=float32)\n",
      "Loss: array(1.5608974, dtype=float32)\n",
      "Loss: array(2.0307987, dtype=float32)\n",
      "Loss: array(1.333953, dtype=float32)\n",
      "Loss: array(2.0420537, dtype=float32)\n",
      "Loss: array(1.7691319, dtype=float32)\n",
      "Loss: array(1.3171262, dtype=float32)\n",
      "Loss: array(1.6162759, dtype=float32)\n",
      "Loss: array(2.2498026, dtype=float32)\n",
      "Loss: array(1.4809728, dtype=float32)\n",
      "Loss: array(1.3100604, dtype=float32)\n",
      "Loss: array(1.8203307, dtype=float32)\n",
      "Loss: array(1.670798, dtype=float32)\n",
      "Loss: array(1.7073382, dtype=float32)\n",
      "Loss: array(1.5493076, dtype=float32)\n",
      "Loss: array(1.4649068, dtype=float32)\n",
      "Loss: array(1.3329848, dtype=float32)\n",
      "Loss: array(1.3474802, dtype=float32)\n",
      "Loss: array(1.4440382, dtype=float32)\n",
      "Loss: array(1.4819286, dtype=float32)\n",
      "Loss: array(1.6311135, dtype=float32)\n",
      "Loss: array(1.4784973, dtype=float32)\n",
      "Loss: array(1.1206886, dtype=float32)\n",
      "Loss: array(1.3419902, dtype=float32)\n",
      "Loss: array(1.4278505, dtype=float32)\n",
      "Loss: array(1.2896328, dtype=float32)\n",
      "Loss: array(1.6055743, dtype=float32)\n",
      "Loss: array(1.435296, dtype=float32)\n",
      "Loss: array(1.8900077, dtype=float32)\n",
      "Loss: array(1.2178992, dtype=float32)\n",
      "Loss: array(1.5011771, dtype=float32)\n",
      "Loss: array(1.5832444, dtype=float32)\n",
      "Loss: array(1.2446495, dtype=float32)\n",
      "Loss: array(1.4039826, dtype=float32)\n",
      "Loss: array(1.4516857, dtype=float32)\n",
      "Loss: array(1.596345, dtype=float32)\n",
      "Loss: array(1.5493242, dtype=float32)\n",
      "Loss: array(1.3593652, dtype=float32)\n",
      "Loss: array(1.9223218, dtype=float32)\n",
      "Loss: array(0.9760767, dtype=float32)\n",
      "Loss: array(0.9453834, dtype=float32)\n",
      "Loss: array(1.4386629, dtype=float32)\n",
      "Loss: array(1.5366703, dtype=float32)\n",
      "Loss: array(1.3872606, dtype=float32)\n",
      "Loss: array(1.6117297, dtype=float32)\n",
      "Loss: array(1.1354862, dtype=float32)\n",
      "Loss: array(1.6859406, dtype=float32)\n",
      "Loss: array(1.1555485, dtype=float32)\n",
      "Loss: array(1.4766793, dtype=float32)\n",
      "Loss: array(1.8367895, dtype=float32)\n",
      "Loss: array(1.2165151, dtype=float32)\n",
      "Loss: array(1.5910383, dtype=float32)\n",
      "Loss: array(1.3854055, dtype=float32)\n",
      "Loss: array(1.274216, dtype=float32)\n",
      "Loss: array(1.4073373, dtype=float32)\n",
      "Loss: array(1.1485255, dtype=float32)\n",
      "Loss: array(1.4841077, dtype=float32)\n",
      "Loss: array(1.2863739, dtype=float32)\n",
      "Loss curve exported as loss_curve.dxf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 4.857s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7230d2eb8fe0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from tinygrad.nn.optim import SGD\n",
    "import tinygrad as tngrd\n",
    "\n",
    "class TestAttentionHead(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.embed_dim = 16\n",
    "        self.head_dim = 8\n",
    "        self.batch_size = 2\n",
    "        self.seq_length = 4\n",
    "        self.attention_head = AttentionHead(self.embed_dim, self.head_dim, requires_grad=True, dtype=tngrd.dtypes.float16)\n",
    "\n",
    "    def test_linear_projection_shapes(self):\n",
    "        x = Tensor(np.random.randn(self.batch_size, self.seq_length, self.embed_dim).astype(np.float32))\n",
    "        output = self.attention_head(x)\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.seq_length, self.embed_dim))\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        x = Tensor(np.random.randn(self.batch_size, self.seq_length, self.embed_dim).astype(np.float32))\n",
    "        output = self.attention_head(x)\n",
    "        self.assertIsNotNone(output)\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.seq_length, self.embed_dim))\n",
    "\n",
    "    def test_gradient_check(self):\n",
    "        x = Tensor(np.random.randn(self.batch_size, self.seq_length, self.embed_dim).astype(np.float32), requires_grad=True)\n",
    "        output = self.attention_head(x)\n",
    "        target = Tensor(np.random.randn(self.batch_size, self.seq_length, self.embed_dim).astype(np.float32))\n",
    "        loss = ((output - target) ** 2).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        self.assertIsNotNone(x.grad)\n",
    "        self.assertEqual(self.attention_head.W_q.grad.shape, self.attention_head.W_q.shape)\n",
    "\n",
    "    def test_training(self):\n",
    "        # Enable training mode\n",
    "        Tensor.training = True  # Make sure Tensor.training is enabled for the optimizer\n",
    "\n",
    "        # Test training over multiple iterations with gradient descent\n",
    "        optimizer = SGD([self.attention_head.W_q, self.attention_head.W_k, self.attention_head.W_v, self.attention_head.W_o,\n",
    "                        self.attention_head.b_q, self.attention_head.b_k, self.attention_head.b_v, self.attention_head.b_o], lr=0.01)\n",
    "\n",
    "        loss_values = []  # Track loss values for plotting\n",
    "\n",
    "        for _ in range(200):\n",
    "            x = Tensor(np.random.randn(self.batch_size, self.seq_length, self.embed_dim).astype(np.float32), requires_grad=True)\n",
    "            target = Tensor(np.random.randn(self.batch_size, self.seq_length, self.embed_dim).astype(np.float32))\n",
    "            output = self.attention_head(x)\n",
    "            loss = ((output - target) ** 2).mean()\n",
    "            loss_values.append(loss.numpy())  # Store the loss value\n",
    "\n",
    "            print(\"Loss:\", repr(loss.numpy()))\n",
    "\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Zero the gradients for the next step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Assert that loss is a scalar\n",
    "            self.assertEqual(loss.shape, ())\n",
    "\n",
    "        # After training, export the loss values to DXF\n",
    "        export_loss_to_dxf(loss_values, filename=\"loss_curve.dxf\")\n",
    "\n",
    "        # Disable training mode after test\n",
    "        Tensor.training = False  # Reset training mode after test\n",
    "\n",
    "# Run the tests\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-03 18:30:35.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1m\u001b[36mTraining model 50/50\u001b[0m\u001b[0m\n",
      "\n",
      "Training Steps:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Training Steps:   3%|▎         | 6/200 [00:00<00:03, 57.19it/s]\n",
      "Training Steps:   6%|▌         | 12/200 [00:00<00:03, 56.49it/s]\n",
      "Training Steps:   9%|▉         | 18/200 [00:00<00:03, 57.98it/s]\n",
      "Training Steps:  12%|█▏        | 24/200 [00:00<00:03, 53.84it/s]\n",
      "Training Steps:  15%|█▌        | 30/200 [00:00<00:03, 54.53it/s]\n",
      "Training Steps:  18%|█▊        | 36/200 [00:00<00:02, 56.18it/s]\n",
      "Training Steps:  22%|██▏       | 43/200 [00:00<00:02, 57.60it/s]\n",
      "Training Steps:  25%|██▌       | 50/200 [00:00<00:02, 58.58it/s]\n",
      "Training Steps:  28%|██▊       | 57/200 [00:00<00:02, 59.19it/s]\n",
      "Training Steps:  32%|███▏      | 64/200 [00:01<00:02, 59.64it/s]\n",
      "Training Steps:  36%|███▌      | 71/200 [00:01<00:02, 60.18it/s]\n",
      "Training Steps:  39%|███▉      | 78/200 [00:01<00:02, 60.70it/s]\n",
      "Training Steps:  42%|████▎     | 85/200 [00:01<00:02, 40.00it/s]\n",
      "Training Steps:  46%|████▌     | 91/200 [00:01<00:02, 43.85it/s]\n",
      "Training Steps:  48%|████▊     | 97/200 [00:01<00:02, 47.27it/s]\n",
      "Training Steps:  52%|█████▏    | 103/200 [00:01<00:01, 50.19it/s]\n",
      "Training Steps:  55%|█████▌    | 110/200 [00:02<00:01, 53.04it/s]\n",
      "Training Steps:  58%|█████▊    | 116/200 [00:02<00:01, 54.67it/s]\n",
      "Training Steps:  62%|██████▏   | 123/200 [00:02<00:01, 56.38it/s]\n",
      "Training Steps:  65%|██████▌   | 130/200 [00:02<00:01, 57.65it/s]\n",
      "Training Steps:  68%|██████▊   | 136/200 [00:02<00:01, 54.06it/s]\n",
      "Training Steps:  71%|███████   | 142/200 [00:02<00:01, 55.53it/s]\n",
      "Training Steps:  74%|███████▍  | 149/200 [00:02<00:00, 57.11it/s]\n",
      "Training Steps:  78%|███████▊  | 155/200 [00:02<00:00, 51.21it/s]\n",
      "Training Steps:  80%|████████  | 161/200 [00:03<00:01, 38.08it/s]\n",
      "Training Steps:  84%|████████▍ | 168/200 [00:03<00:00, 43.35it/s]\n",
      "Training Steps:  87%|████████▋ | 174/200 [00:03<00:00, 46.75it/s]\n",
      "Training Steps:  90%|█████████ | 181/200 [00:03<00:00, 50.50it/s]\n",
      "Training Steps:  94%|█████████▎| 187/200 [00:03<00:00, 51.89it/s]\n",
      "Training Steps:  97%|█████████▋| 194/200 [00:03<00:00, 54.37it/s]\n",
      "Training Steps: 100%|██████████| 200/200 [00:03<00:00, 55.76it/s]\n",
      "                                                                 \u001b[32m2024-10-03 18:30:39.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mModel 50 finished with final loss: \u001b[33m1.1510272026062012\u001b[0m\u001b[0m\n",
      "Training Models: 100%|██████████| 50/50 [03:09<00:00,  3.79s/it]\n"
     ]
    }
   ],
   "source": [
    "import os  # For clearing the terminal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tinygrad.nn.optim import SGD\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from loguru import logger  # For logging\n",
    "from colorama import Fore, Style  # For colored terminal output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Initialize Colorama for colored text\n",
    "from colorama import init\n",
    "init(autoreset=True)\n",
    "\n",
    "# Customize logger with colored output using colorama\n",
    "logger.add(lambda msg: print(f\"{Fore.GREEN}{msg}{Style.RESET_ALL}\"))\n",
    "\n",
    "# Training function for attention head\n",
    "def train_attention_head(attention_head, num_iterations=200, lr=0.01):\n",
    "    # Enable training mode\n",
    "    Tensor.training = True  # Make sure Tensor.training is enabled for the optimizer\n",
    "\n",
    "    optimizer = SGD([attention_head.W_q, attention_head.W_k, attention_head.W_v, attention_head.W_o,\n",
    "                     attention_head.b_q, attention_head.b_k, attention_head.b_v, attention_head.b_o], lr=lr)\n",
    "\n",
    "    loss_values = []\n",
    "    for _ in tqdm(range(num_iterations), desc=\"Training Steps\", leave=False):\n",
    "        x = Tensor(np.random.randn(batch_size, seq_length, embed_dim).astype(np.float32), requires_grad=True)\n",
    "        target = Tensor(np.random.randn(batch_size, seq_length, embed_dim).astype(np.float32))\n",
    "        output = attention_head(x)\n",
    "        loss = ((output - target) ** 2).mean()\n",
    "        loss_values.append(loss.numpy())\n",
    "\n",
    "        # Backward pass and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Return final loss and concatenation of all weights\n",
    "    final_loss = loss.numpy()\n",
    "    weights_vector = np.concatenate([attention_head.W_q.numpy().flatten(),\n",
    "                                     attention_head.W_k.numpy().flatten(),\n",
    "                                     attention_head.W_v.numpy().flatten(),\n",
    "                                     attention_head.W_o.numpy().flatten(),\n",
    "                                     attention_head.b_q.numpy(),\n",
    "                                     attention_head.b_k.numpy(),\n",
    "                                     attention_head.b_v.numpy(),\n",
    "                                     attention_head.b_o.numpy()])\n",
    "\n",
    "    # Disable training mode\n",
    "    Tensor.training = False  # Reset training mode\n",
    "\n",
    "    return final_loss, weights_vector\n",
    "\n",
    "# Hyperparameters\n",
    "num_models = 50\n",
    "embed_dim = 16\n",
    "head_dim = 8\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "\n",
    "# Train multiple AttentionHead models and collect weight vectors and losses\n",
    "weight_vectors = []\n",
    "losses = []\n",
    "\n",
    "logger.info(f\"Starting training of {num_models} models\")\n",
    "\n",
    "# Outer tqdm loop for training multiple models\n",
    "for i in tqdm(range(num_models), desc=\"Training Models\"):\n",
    "    clear_output(wait=True)\n",
    "    # Clear the terminal before logging each model's details\n",
    "    os.system('cls' if os.name == 'nt' else 'clear')\n",
    "\n",
    "    logger.info(f\"{Fore.CYAN}Training model {i+1}/{num_models}{Style.RESET_ALL}\")\n",
    "    attention_head = AttentionHead(embed_dim, head_dim, requires_grad=True, dtype=tngrd.dtypes.float16)\n",
    "\n",
    "    final_loss, weights_vector = train_attention_head(attention_head)\n",
    "    weight_vectors.append(weights_vector)\n",
    "    losses.append(final_loss)\n",
    "\n",
    "    logger.info(f\"Model {i+1} finished with final loss: {Fore.YELLOW}{final_loss}{Style.RESET_ALL}\")\n",
    "\n",
    "# Convert to numpy arrays for dimensionality reduction\n",
    "weight_vectors = np.array(weight_vectors)\n",
    "losses = np.array(losses)\n",
    "\n",
    "# Standardize the weight vectors\n",
    "scaler = StandardScaler()\n",
    "weight_vectors_std = scaler.fit_transform(weight_vectors)\n",
    "\n",
    "# Apply PCA (or TSNE or UMAP) for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "reduced_weights = pca.fit_transform(weight_vectors_std)\n",
    "\n",
    "# Create a 2D heatmap (or 3D plot with loss values)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_weights[:, 0], reduced_weights[:, 1], c=losses, cmap='coolwarm')\n",
    "plt.colorbar(label='Final Loss')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Solution Space of AttentionHead Models')\n",
    "plt.show()\n",
    "\n",
    "logger.info(f\"{Fore.GREEN}Training completed and plot generated!{Style.RESET_ALL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
